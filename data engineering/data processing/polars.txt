https://motional.com/news/polar-stream-simultaneous-object-detection-and-semantic-segmentation-algorithm-streaming-lidar
Why Polars:
	->Can do common operations around 5–10 times faster than pandas.
	->Pandas requires around 5 to 10 times as much RAM as the size of the dataset to carry out operations, compared to the 2 to 4 times needed for Polars.
	->Polars can work fine upto 200 gigabytes of data on a single machine. If you go into terabyte scale we should use pySpark.
	->Polar focuses on single machine optimal performance and efficient use of resources.
	->Adheres to a strict schema (data-types should be known before running the query).
	->Support streaming.
	->The streaming API allows you to process your results without requiring all your data to be in memory at the same time.
	=>Polars can execute queries on a streaming engine, allowing for processing large datasets without loading everything into memory at once.
		-Polars supports executing lazy queries in streaming mode by passing streaming=True to the .collect() method.
		-The streaming engine breaks down queries into batches, allowing for efficient processing of large datasets that don't fit into memory.
		-Not All Operations Supported. If not supported will fall back to non-streaming execution.
	->Streaming APIs create a continuous connection to share live data with the user until they close that connection, without significant latency.
->Why polars is faster:
	=Written in Rust: 
		-While pandas uses numpy(written in C) it faces problems with the way python handles certain types in memory, such as strings.
		-Another adv of using it allows for safe concurrency(parallelism).
		-Polars is designed to utilize all available CPU cores for parallel execution, not just one core, making it significantly faster than libraries like pandas which primarily use a single core.
	=Based on Arrow:
		-It is also the backend for pandas 2.0.
		-It is called Apache Arrow, a language-independent memory format.
		-Main advantages of building a data library on Arrow is interoperability. It has been designed to standardize the in-memory data format used across libraries(pandas, spark, ....). So we can switch to pandas from polars anytime to do any task as we just need to pass an arrow memory buffer between them.
		-This interoperability speeds up performance as it bypasses the need to convert data into a different format to pass it between different steps of the data pipeline (in other words, it avoids the need to serialize and deserialize the data as serialization/deserialization is estimated to represent 80–90% of the computing costs in data workflows, Arrow’s common data format lends Polars significant performance gains).
		-Arrow also has built-in support for a wider range of data types(pandas excels to handle int, float but not at others) inc datetime, boolean, binary, and even complex column types, such as those containing lists.
		-Arrow able to natively handle missing data, which requires a workaround in NumPy.
		-Here columns are stored in a continuous block of memory.
	=Query optimization:
		-Pandas, by default, uses eager execution, carrying out operations in the order you’ve written them.
		-But polar can do both eager and lazy exec.
			-Lazy exec: Evaluate all of the required operations and map out the most efficient way of executing the code. This can include, among other things, rewriting the execution order of operations or dropping redundant calculations.
		-Reduces unnecessary work and memory allocations
	=Expressive API:
		-Meaning that basically any operation you want to perform can be expressed as a Polars method.
		-More complex operations in pandas often need to be passed to the apply method as a lambda expression here we loop through each row of DF sequentially executing the operation on each one.
		-Being able to use built-in methods allows you to work on a columnar level and take advantage of another form of parallelism called SIMD.

Using Polars:
->Polar expressions: They provide a way of expressing data transformations. Eg: pl.col("weight") / (pl.col("height") ** 2)
->Polar Context: Polars expressions need a context in which they are executed to produce a result. Depending on the context it is used in, the same Polars expression can produce different results. Types: select(select and manipulate cols), with_columns(adds cols), filter, group_by.
->We need to use polars expression API's to use it efficiently.
	-1st advantage is it let's us transform data or do operations while fetching data.
	-2nd it takes care of running operations in parallel.
	-3rd it lets us run ops in lazy mode.
->Series is a 1-dimensional homogeneous data structure.
->A dataframe is a 2-dimensional heterogeneous data structure that contains uniquely named series.
->Polars uses null to represent missing values
->In Polars NaN values are treated as valid floating-point values
->In polars:
	-Any NaN compares equal to other NaN value. And greater than any non NaN values.
->List vs Array in polars:
	-list and array both stores homogenous data in them.
	-We use array when we know the size of it Eg: pl.Array(pl.Boolean, 5).
	==prefer the data type Array over List because it is more memory efficient and more performant. If you cannot use Array, then use List
->modin is library that makes pandas run faster in parallel used to optimize pandas.
->using engine='pyarrow' in read_csv_optimized code significantly.
->We can access data from polars df using df[3:,[col1,col2]] but to use full power of polars we should use expression API.
	-Eg: df.select([pl.col(col1), pl.col(col2)])
	-It lets us perform transformation on cols.
	-It takes care of running operations in parallel.
	-Lets us use query optimization in lazy mode.
->In lazy mode polars doesn't run each command line by line but gathers full query to run it in one go. It auto optimizes query/ops on data.
	-Use .describe_optimized_plan() to get the optimized query from polars.
	-We use scan_csv() instead of read csv to perform ops in lazy mode.
	-Eg: print(pl.scan_csv(file1)).groupBy(['col1', 'col2']).agg(pl.col("col3").count().alias("al1")).describe_optimized_plan()
		-Prints query where it gets only required cols from file and perform op.
->Streamin:
	-Streaming is where polars runs large data sets in batches so we can work with data sets much larger than memory.
	-We use streaming in lazy mode in polars.
		-To use lazy mode without streaming we use .collect() at end of query.
		-To use lazy mode with streaming we use .collect(streaming=True) at end of query.
->The lazyframe is query plan to execute ops we need to use .collect for cops to execute and get data frame from it.
->.lazy() on dataFrames turns it to lazyFrame. 
->Df to series:
	-df['col1']
	-df.select('col1').to_series()
->Series to DF:
	-s.to_frame()
->Series:
	-values = [1,2,3]
	 s = pl.Series(name='col1', values=values)
	-s.to_list()
->DataFrame:
	->DF from dict.
		-data = {'a': [1,2], 'b': [2,3]}
		 pl.DataFrame(data)
	->DF from lists.
		-data=[[1,2][2,3]]
	 	 pl.DataFrame(data, schema=['col1', 'col2'])
	->Schema
		-data={'a': [1,2], 'b': [2,3]}
		 pl.DataFrame(data, schema=data)
	->dF to list of dicts
		-df.to_dict()
	->DF from numpy.
		-data = np.random.standard_normal((5, 3))
		 pl.DataFrame(data)
	->pandas DF from polars df.
		-pd.DataFrame(pl_df)
	->df.to_numpy()
		-Creates numpy array where each array is a row of DF.
		-df['col1'].to_numpy() #series to numpy
		-df['col1'].to_numpy(zero_copy_only=True) #this won't create copy of data as numpy array instead refers data. Doing this we can't perform ops to modify numpy array. And we should not have null or nans in data.
	->df.to_pandas()
		-This conversion requires installing pyarrow lib.
		-This clones data similar to calling to_numpy on DF.
		-df.to_pandas(use_pyarrow_extension_array=True) #pyArrow backed pandas.
			-Here we create pandas data frame that refers same arrow table as polars DF. This means you can use pandas code on data with out 			 copying it.
			-This is very cheap as it does not copy whole data.
	->.value_count() on series created DF which contain 2 cols value in series and count of each value.